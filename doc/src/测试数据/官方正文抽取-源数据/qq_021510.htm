腾讯科技讯（刘亚澜）5月24日，在新版本AlphaGo首战以1/4子微弱优势战胜中国围棋职业九段棋手柯洁之后，“AlphaGo之父”DeepMind创始人兼CEO Demis Hassabis、AlphaGo团队负责人David Silver在人工智能高峰论坛上详解了AlphaGo的研发并就“AlphaGo意味着什么？”的问题进行了详细解答。“AlphaGo已经展示出了创造力，也已经可以模仿人类直觉了。在过去一年，我们继续打造AlphaGo，我们想打造完美的AlphaGo，弥补它知识方面的空白。因为在与李世石的比赛中，它是有缺陷的。”Demis Hassabis说：“在未来我们能看到人机合作的巨大力量，人类智慧将通过人工智能进一步放大。强人工智能是人类研究和探寻宇宙的终极工具。”为什么计算机下围棋非常困难？Demis Hassabis坦言围棋非常困难，因为其复杂程度让穷举搜索都难以解决。对于计算机来说，围棋有两项难题：“不可能”写出评估程序以决定谁赢，搜索空间太过庞大。围棋不像象棋等游戏靠计算，而是靠直觉。围棋中没有等级概念，所有棋子都一样。围棋是筑防游戏，因此需要盘算未来。小小一子可撼全局，“妙手”如受天启。AlphaGo如何进行训练？David Silver从技术角度详细解释了AlphaGo如何进行训练。围棋对于机器的难点之一是评估程序的撰写。而AlphaGo团队用两种卷积神经网络去完成：策略网络和估值网络。策略网络的卷积神经网络用于决定下一步落子可能的位置，价值网络用于评估当前棋局获胜的概率。为了应对围棋的巨大复杂性，AlphaGo 采用机器学习技术，结合了监督学习和强化学习的优势。通过训练形成一个策略网络（policy network），将棋盘上的局势作为输入信息，并对所有可行的落子位置生成一个概率分布。然后，训练出一个价值网络（value network）对自我对弈进行预测，以 -1（对手的绝对胜利）到1（AlphaGo的绝对胜利）的标准，预测所有可行落子位置的结果。这两个网络自身都十分强大，而 AlphaGo将这两种网络整合进基于概率的蒙特卡罗树搜索（MCTS）中，实现了它真正的优势。最后，新版的AlphaGo 产生大量自我对弈棋局，为下一代版本提供了训练数据，此过程循环往复。AlphaGo 如何决定落子？在获取棋局信息后，AlphaGo会根据策略网络探索哪个位置同时具备高潜在价值和高可能性，进而决定最佳落子位置。在分配的搜索时间结束时，模拟过程中被系统最频繁考察的位置将成为 AlphaGo的最终选择。在经过先期的全盘探索和过程中对最佳落子的不断揣摩后，AlphaGo的搜索算法就能在其计算能力之上加入近似人类的直觉判断。David Silver总结：策略网络减少宽度，价值网络减少深度。AlphaGo做出多种模拟，不断反复，最终形成判断哪种方案是获胜概率最高的。今年的AlphaGo和去年的AlphaGo有什么区别？David Silver透露，去年的AlphaGo Lee在云上有50TPUs在运作，搜索50个棋步为10000个位置／秒。而今年的AlphaGo Master是在单个TPU机器上进行游戏，它已经成为了自己的老师，从自己的搜索里学习，拥有更强大的策略网络和价值网络。AlphaGo如何进行自我学习？Demis Hassabis将AlphaGo归类为强人工智能，强人工智能和弱人工智能的区别在于弱人工智能是预设置的，例如IBM的“深蓝”就不能自我学习。他提到强化学习框架的概念：智能体有一个特定目标要完成，它有两种方式和环境打交道，一是观察，智能体通过观察进行见面，这有可能不全面。二是行动。David Silver称，AlphaGo先自己与自己对弈，策略网络以P预测AlphaGo的移动。人工智能的元解决方案Demis Hassabis表示，目前信息过载和系统冗杂是人类面临的巨大挑战。开发人工智能技术可能是这些问题的元解决方案。元解决方案的目标是实现“人工智能科学家”或“人工智能辅助科学”。“人工智能和所有强大的新技术一样，在伦理和责任的约束中造福人类。